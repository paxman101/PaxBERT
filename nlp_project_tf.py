"""NLP Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1avJo4M_jIaZ0jw27aOVDSIRcDK385jyC

NLP with review classification
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf
import logging
import os

import transformers as ppb
from collections import Counter

logger = logging.getLogger(__name__)

class SavePretrainedCallback(tf.keras.callbacks.Callback):
    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary
    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback
    # that saves the model with this method after each epoch.
    def __init__(self, output_dir, **kwargs):
        super().__init__()
        self.output_dir = output_dir

    def on_epoch_end(self, epoch, logs=None):
        self.model.save_pretrained(self.output_dir)


"""#Dataset Preprocessing 

Read first 4000 rows from amazon dataset

"""

#df = pd.read_csv('books_v1_02_cleaned.tsv.gz', compression='gzip', sep='\t')
df = pd.read_csv('out.csv', nrows=20)

df = df.astype({'star_rating': float})
df['star_rating'] = (df['star_rating'] - 1) / 4.

train_texts, val_texts, train_labels, val_labels = train_test_split(df.review_body.values, df.star_rating.values,
                                                                    test_size=.2)
test_texts, val_texts, test_labels, val_labels = train_test_split(val_texts, val_labels, test_size=.5)

model_class, tokenizer_class, pretrained_weights = (
    ppb.TFDistilBertForSequenceClassification, ppb.DistilBertTokenizerFast, 'distilbert-base-uncased')  # for trainer API

config = ppb.DistilBertConfig.from_pretrained(pretrained_weights, num_labels=1, problem_type="regression")
tokenizer = tokenizer_class.from_pretrained(pretrained_weights, config=config)
model = model_class.from_pretrained(pretrained_weights, config=config)

train_encodings = tokenizer(train_texts.tolist(), padding=True, truncation=True)
val_encodings = tokenizer(val_texts.tolist(), padding=True, truncation=True)
test_encodings = tokenizer(test_texts.tolist(), padding=True, truncation=True)

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(train_labels)))
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), list(val_labels)))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), list(test_labels)))

print(train_dataset.element_spec)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss_fn = tf.keras.losses.MeanSquaredError()
metrics = ['RootMeanSquaredError']

model.compile(optimizer=optimizer,
              loss=loss_fn,
              metrics=metrics)

BATCH_SIZE = 32
N_EPOCHS = 3
OUTPUT_DIR = "./output"

callbacks = [SavePretrainedCallback(output_dir=OUTPUT_DIR)]
model.fit(train_dataset.shuffle(len(train_texts)).batch(BATCH_SIZE),
          validation_data=val_dataset.batch(BATCH_SIZE),
          epochs=N_EPOCHS,
          batch_size=BATCH_SIZE,
          callbacks=callbacks)

logger.info("Predictions on test dataset...")
predictions = model.predict(test_dataset.batch(BATCH_SIZE))
out_test_file = os.path.join(OUTPUT_DIR, "test_results.txt")
with open(out_test_file, "w") as writer:
    writer.write("index\tprediction\n")
    for ele in test_dataset.enumerate().as_numpy_iterator():
        writer.write(str(ele))
    logger.info(f"Wrote predictions to {out_test_file}")