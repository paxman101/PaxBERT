"""NLP Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1avJo4M_jIaZ0jw27aOVDSIRcDK385jyC

NLP with review classification
"""

import tensorflow as tf
from tensorflow import TensorSpec
import logging
import os
import horovod.tensorflow.keras as hvd
import transformers as ppb

hvd.init()

DATASET_TENSORSPEC = ({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None),
                       'token_type_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None),
                       'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)},
                      TensorSpec(shape=(), dtype=tf.float64, name=None))


gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')


logger = logging.getLogger(__name__)


class SavePretrainedCallback(tf.keras.callbacks.Callback):
    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary
    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback
    # that saves the model with this method after each epoch.
    def __init__(self, output_dir, **kwargs):
        super().__init__()
        self.output_dir = output_dir

    def on_epoch_end(self, epoch, logs=None):
        self.model.save_pretrained(self.output_dir.format(epoch=epoch))


model_class, tokenizer_class, pretrained_weights = (
    ppb.TFBertForSequenceClassification, ppb.BertTokenizerFast,
    'bert-base-uncased')  # for trainer API

config = ppb.BertConfig.from_pretrained(pretrained_weights, num_labels=1, problem_type="regression")
tokenizer = tokenizer_class.from_pretrained(pretrained_weights, config=config)
model = model_class.from_pretrained(pretrained_weights, config=config)

train_dataset = tf.data.experimental.load(f"./dataset_shards/train_dataset_{hvd.rank()}", element_spec=DATASET_TENSORSPEC, compression="GZIP")
val_dataset = tf.data.experimental.load(f"./dataset_shards/val_dataset_{hvd.rank()}", element_spec=DATASET_TENSORSPEC, compression="GZIP")
if hvd.rank() == 0:
    test_dataset = tf.data.experimental.load("./dataset_shards/test_dataset_0", element_spec=DATASET_TENSORSPEC, compression="GZIP")
    for i in range(1, 8):
        in_data = tf.data.experimental.load(f"./dataset_shards/test_dataset_{i}", element_spec=DATASET_TENSORSPEC, compression="GZIP")
        test_dataset = test_dataset.concatenate(in_data)


optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5 * hvd.size())
optimizer = hvd.DistributedOptimizer(optimizer)

loss_fn = tf.keras.losses.MeanSquaredError()
metrics = ['RootMeanSquaredError']

model.compile(optimizer=optimizer,
              loss=loss_fn,
              metrics=metrics,
              experimental_run_tf_function=False)

BATCH_SIZE = 8
N_EPOCHS = 3
OUTPUT_DIR = "./output"

callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]

if hvd.rank() == 0:
    callbacks.append(SavePretrainedCallback(output_dir="./checkpoints/checkpoint-{epoch}.h5"))
num_train_samples = 309624

model.fit(train_dataset.batch(BATCH_SIZE),
          validation_data=val_dataset.batch(BATCH_SIZE),
          epochs=N_EPOCHS,
          batch_size=BATCH_SIZE,
          callbacks=callbacks,
          verbose=1 if hvd.rank() == 0 else 0,
          steps_per_epoch=num_train_samples // BATCH_SIZE)

if hvd.rank() == 0:
    logger.info("Predictions on test dataset...")
    predictions = model.predict(test_dataset.batch(batch_size=BATCH_SIZE))
    out_test_file = os.path.join(OUTPUT_DIR, "test_results.txt")
    with open(out_test_file, "w") as writer:
        writer.write(str(predictions.to_tuple()))
        for ele in test_dataset.enumerate().as_numpy_iterator():
            writer.write(str(ele))
        logger.info(f"Wrote predictions to {out_test_file}")
